{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T18:39:36.505187Z",
     "start_time": "2021-10-07T18:39:36.404141Z"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "If didn't see the `d2l` environment in your existing kernels (by clicking `Change kernel`), you can setup the environment now: simply click on <button data-commandlinker-command=\"conda env create -f env.yml\" data-commandlinker-args=\"{&quot;url&quot;: &quot;https://github.com/d2l-ai/d2l-en&quot;}\">Install D2L Environmnet</button>\n",
    ", and you will create a new environment named `d2l` with all the required Python packages (as listed in `env.yml`). The `d2l` environment will be used by all the jupyter notebooks automatically.\n",
    "\n",
    "\n",
    "## About D2L\n",
    "\n",
    "Dive into Deep Learning ([d2l.ai](https://www.d2l.ai/)) is an interactive deep learning book with code, math, and discussions. Over 200 universities, including more than half of the top 30 universities (e.g., Stanford University, Massachusetts Institute of Technology, Carnegie Mellon University, Tsinghua University) adopt D2L as a course or supplemental material. \n",
    "\n",
    "\n",
    "### Content and Structure\n",
    "\n",
    "The book can be roughly divided into three parts,\n",
    "focusing on preliminaries, deep learning techniques,\n",
    "and advanced topics focused on real systems \n",
    "and applications.\n",
    "\n",
    "![Book structure](https://d2l.ai/_images/book-org.svg)\n",
    "\n",
    "\n",
    "* The first part covers basics and preliminaries.\n",
    "Chapter *Introduction* offers \n",
    "an introduction to deep learning.\n",
    "Then, via chapter *Preliminaries*,\n",
    "we quickly bring you up to speed \n",
    "on the prerequisites required\n",
    "for hands-on deep learning, \n",
    "such as how to store and manipulate data,\n",
    "and how to apply various numerical operations \n",
    "based on basic concepts from linear algebra, \n",
    "calculus, and probability.\n",
    "chapter *Linear Neural Networks* and chapter *Multilayer Perceptrons* cover the most basic concepts \n",
    "and techniques in deep learning,\n",
    "including regression and classification;\n",
    "linear models and multilayer perceptrons;\n",
    "and overfitting and regularization.\n",
    "\n",
    "* The next five chapters focus on modern deep learning techniques.\n",
    "Chapter *Deep Learning Computation* describes \n",
    "the key computational components \n",
    "of deep learning systems\n",
    "and lays the groundwork\n",
    "for our subsequent implementations\n",
    "of more complex models.\n",
    "Next, chapter *Convolutional Neural Networks* and \n",
    "chapter *Modern Convolutional Neural Networks* \n",
    "introduce convolutional neural networks (CNNs), \n",
    "powerful tools that form the backbone \n",
    "of most modern computer vision (CV) systems.\n",
    "Similarly, chapter *Recurrent Neural Networks*  and \n",
    "chapter *Modern Recurrent Neural Networks* \n",
    "introduce recurrent neural networks (RNNs), \n",
    "models that exploit sequential (e.g., temporal) \n",
    "structure in data and are commonly used\n",
    "for natural language processing (NLP) \n",
    "and time series prediction.\n",
    "In chapter *Attention Mechanisms*, \n",
    "we introduce a relatively new class of models\n",
    "based on attention and transformer mechanisms\n",
    "as the dominant architecture\n",
    "for most NLP tasks.\n",
    "These sections will bring you up to speed \n",
    "on the most powerful and general tools\n",
    "that are widely used by deep learning practitioners.\n",
    "\n",
    "* Part three discusses scalability, efficiency, and applications.\n",
    "First, in chapter *Optimization Algorithms*,\n",
    "we discuss several common optimization methods\n",
    "used to train deep learning models.\n",
    "The next chapter *Computational Performance* \n",
    "examines several key factors\n",
    "that influence the computational performance \n",
    "of your deep learning code.\n",
    "In chap *Computer Vision*,\n",
    "we illustrate major applications \n",
    "of deep learning in CV.\n",
    "In chapter *Natural Language Processing: Pretraining* \n",
    "and chapter *Natural Language Processing: Applications*,\n",
    "we show how to pretrain language representation models \n",
    "and apply them to NLP tasks.\n",
    "\n",
    "## CPU or GPU?\n",
    "\n",
    "D2L provides a conbination of \n",
    "computational light and heavy notebooks. \n",
    "To study the first six chapters, \n",
    "we encourage you to use CPU; \n",
    "while studying the rest chapters, \n",
    "you may benefit from GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T18:42:40.734356Z",
     "start_time": "2021-10-07T18:42:40.723603Z"
    }
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "This book is for students (undergraduate or graduate),\n",
    "engineers, and researchers, who seek a solid grasp\n",
    "of the practical techniques of deep learning.\n",
    "Because we explain every concept from scratch,\n",
    "no previous background in deep learning or machine learning is required.\n",
    "Fully explaining the methods of deep learning\n",
    "requires some mathematics and programming,\n",
    "but we will only assume that you come in with some basics,\n",
    "including modest amounts of linear algebra, \n",
    "calculus, probability, and Python programming.\n",
    "Just in case you forget the basics,\n",
    "the Appendix chapter ( :numref:`chap_appendix_math`) provides a refresher \n",
    "on most of the mathematics \n",
    "you will find in this book.\n",
    "Most of the time, we will prioritize \n",
    "intuition and ideas\n",
    "over mathematical rigor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning by Doing\n",
    "\n",
    "Most sections of this book feature executable code.\n",
    "We believe that some intuitions are best developed\n",
    "via trial and error,\n",
    "tweaking the code in small ways and observing the results.\n",
    "\n",
    "To avoid unnecessary repetition, we encapsulate \n",
    "some of our most frequently imported and referred-to \n",
    "functions and classes in the `d2l` package.\n",
    "To indicate a block of code, such as a function, \n",
    "class, or collection of import statements,\n",
    "that will be subsequently accessed via the `d2l` package, \n",
    "we will mark it with `#@save`. \n",
    "We offer a detailed overview \n",
    "of these functions and classes in :numref:`sec_d2l`.\n",
    "The `d2l` package is lightweight and only requires\n",
    "the following dependencies:\n",
    "\n",
    "Most of the code in this book is based on PyTorch,\n",
    "an extremely popular open-source framework\n",
    "that has been enthusiastically embraced \n",
    "by the deep learning research community.\n",
    "All of the code in this book has passed tests \n",
    "under the latest stable verion of PyTorch.\n",
    "However, due to the rapid development of deep learning,\n",
    "some code *in the repository* \n",
    "may not work properly in future versions of PyTorch.\n",
    "We plan to keep the [online book](https://d2l.ai/) up-to-date.\n",
    "In case you encounter any problems,\n",
    "please consult :ref:`chap_installation`\n",
    "to update your code and runtime environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
